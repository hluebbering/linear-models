---
title: "Model Diagnostics"
output:
  html_document: 
    css: ./assets/styles.css
    toc: yes
    toc_float: yes
    highlight: textmate
bibliography: ./assets/citation/sta312.bib
csl: ./assets/citation/institute-of-mathematical-statistics.csl 
link-citations: no
nocite: '@*'
---


```{r, include=FALSE}
knitr::opts_chunk$set(out.width = 450,fig.align = "center", message = FALSE, warning = FALSE, echo = FALSE)
library(ggplot2)
library(broom)
library(kableExtra)
library(knitr)
library(Rcpp)
library(faraway) 
library(visdat)
```

In this lab we will work with the packages `faraway` and `visdat`. We will work with the data frame `mammalsleep`, from the faraway package. First, we load the data into our environment:

```{r Load-Data, echo=TRUE}
library(faraway)
library(visdat)
data(mammalsleep)
```



$\require{color}\definecolor{teall}{RGB}{58, 171, 174} \definecolor{bluemoon}{RGB}{62, 71, 125}\definecolor{gween}{RGB}{73, 175, 129}$


## <span class="sp11">1. </span> Find the best fitted model for predicting sleep. 


**<span class="lp11">a. </span> Run a visdat analysis on the data:**

We use the `visdat` package to visualize everything inside of the mammalsleep dataframe. From this plot, we can identify each variables' class and whether the values are missing.


```{r Visdat, fig.cap="The visdat output shows the number of missing values for variables nondream, sleep, lifespan, and gestation.", echo=TRUE}
vis_dat(mammalsleep)
```




\ 

**<span class="lp11">b. </span> Fit a squares regression model to predict Sleep:**


Our dataset contains 62 observations, and we'll use the following 7 variables in our model: body weight, brain weight, lifespan, gestation time, prediction index, sleep exposure index, and danger. But first, we remove observations with missing values for the outcome sleep and the predictors lifespan, or gestation. 


```{r Remove-Missing, echo=TRUE}
df <- mammalsleep[(!is.na(mammalsleep$sleep) &
                     !is.na(mammalsleep$lifespan) &
                     !is.na(mammalsleep$gestation)),]
```


```{r lmFit, echo=TRUE}
mod <- lm(sleep ~ body + brain + lifespan + gestation +
            predation + exposure + danger, 
          data = df)
```


\ 

**<span class="lp11">c. </span> Test the model assumptions:**

`Constant Variance Assumption.`


```{r Resid-Fit, fig.cap = "Residuals vs. Fits plot to examine non-linearity and constant variance assumptions."}
check <- data.frame(
  e = resid(mod),
  fit = fitted(mod)
)

ggplot(check, aes(x = fit, y = e)) +
         geom_point() +
         geom_hline(yintercept = 0, 
                    col = "plum") +
  labs(y = "Residuals", x = "Fitted Values") +
  theme_minimal()
```


\ 


From the above Residuals vs. Fits plot, there aren't enough observations to make a sound conclusion. Linearity looks okay. Though, we should be a little worried about the constant variance assumption because it seems the points fan out from a thinner to a wider extent. 


\ 

`Normality Assumption.`

```{r Normality, fig.cap = "Q-Q plot to check normality of the residuals"}
ggplot(check, aes(sample = e)) +
  geom_qq() + 
  geom_qq_line(color="darkseagreen") +
  theme_minimal()
```


\ 


There seems to be a little deviation for the upper plot, which suggests that there are outliers present. So, maybe we should use a different model. 


\ 


**<span class="lp11">d. </span> Test the transformed model assumptions:**

Here, we transform our linear regression by taking the $\small\log$ of the predictor variable as follows.


```{r lmFit-Log, echo=TRUE}
modLog <- lm(log(sleep) ~ body + brain + lifespan + 
               gestation + predation + exposure + 
               danger, data = df)
```


\ 

Now, we test the model assumptions, similar to the above, for the transformed model.

```{r Resid-Fit-Log, fig.cap = "Residuals vs. Fits plot to examine non-linearity and constant variance assumptions and Q-Q plot to check normality of the residuals", fig.show='hold', out.width=330}
checkLog <- data.frame(
  e = resid(modLog),
  fit = fitted(modLog)
)

ggplot(checkLog, aes(x = fit, y = e)) +
         geom_point() +
         geom_hline(yintercept = 0, 
                    col = "plum") +
  labs(y = "Residuals", x = "Fitted Values") +
  theme_minimal()

ggplot(checkLog, aes(sample = e)) +
  geom_qq() + 
  geom_qq_line(color="darkseagreen") +
  theme_minimal()
```


\ 

Looking at the different plots, we can see that there are no significant improvements between the two models. Therefore, we will stick to the original squares regression model. Because we have a relatively small sample of 61 observations, we might violate some of the assumptions. 



\ 

**<span class="lp11">e. </span> Interpret the Model's Coefficients:**

From above, we conclude that the original squares regression model is the better fit as opposed to a transformed model fit. Hence, we re-define our model as follows and evaluate its coefficients using `tidy`.


```{r Final-Model, echo=TRUE}
mod <- lm(sleep ~ body + brain + lifespan + gestation +
            predation + exposure + danger, 
          data = df)

mod_table <- tidy(mod, conf.int = TRUE)
```

\ 

```{r}
knitr::kable(mod_table) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "condensed"), font_size = 11)
```


\ 

The above summarizes information about the components of our final model, displaying each explanatory variables' estimate, standard error, F Statistic, p-value, and the confidence interval bounds. Further assessing the model's coefficients, we create a `Forest-Plot` to visualize the measures of effect of each explanatory variable and their confidence intervals.


\ 

```{r Forest-Plot, fig.cap = "Forest Plot"}
ggplot(mod_table[-1, ], aes(x = term, y = estimate,
                      ymin = conf.low, ymax = conf.high)) +
  geom_pointrange() + 
  geom_hline(yintercept = 0, lty = 2, color = "aquamarine4") +
  coord_flip()
```


\ 

It looks like there are two important variables: predation index and danger index. Based on the above, if we're already including danger and predation, we don't necessarily need to include the sleep exposure index. 


\ 

**<span class="lp11">f. </span> The Final Model:**

Here, we form our final model based on the above analysis of various linear model fits. We choose not to include the sleep exposure index because the danger index is a combination of the predation and exposure index. Therefore, the danger index consists of the information we might gain from exposure.


```{r Updated-Final-Model, echo=TRUE}
mod <- lm(sleep ~ body + brain + lifespan + gestation +
            predation + danger, data = df)

mod_table <- tidy(mod, conf.int = TRUE)
```



```{r}
kable(mod_table, digits = 2, col.names = c("Term", "Estimate", "SE", "Statistic", "p-value", "Lower Bound", "Upper Bound")) %>%
  kable_styling(full_width = FALSE, font_size = 11, bootstrap_options = c("striped", "condensed")) %>%
  add_header_above(c(" " = 5, "95% Confidence Interval" = 2))
```


\ 


The variables body, brain, and lifespan have little impact on sleep duration when adjusting for body, brain, lifespan, gestation, predation, and danger, all with coefficients very close to $\small 0$. 

- A one day increase in lifespan yields an expected change in total sleep hours per day of ${\small -0.03}$ (95% CI: -0.10, 0.04) holding all other variables at constant.
- A one day increase in gestation time yields an expected change in total sleep hours per day of ${\small -0.01}$ (95% CI: -0.03, 0) holding all other variables at constant.
- A one unit change in predation index yields an expected change in total sleep of ${\small 2.18}$ (95% CI: 0.25, 4.1) hours per day holding all other variables constant.
- A one unit change in danger index yields an expected change in total sleep of ${\small -3.83}$ (95% CI: -5.91,	-1.76) hours per day holding all other variables constant. 



$$
\begin{gather}
\color{darkgreen} \hat Y & \equiv {\small 16.49 - .001 X_1 + .002 X_2 - .03 X_3 - .01 X_4 + 2.18 X_5 - 3.83 X_6} & + {\large \varepsilon}_{\mathcal{\mid x,i}}
\end{gather}
$$






-----------------------------


## Checking Model Assumptions



```{r AssumptionsCheck}
mod_table1 <- data.frame("Assumptions" = 
             c("Linearity",
               "Constant Variance",
               "Normality",
               "Correlated Error" ),
           "Checking Assumptions" = 
             c("Residuals vs. Fits Plot",
               "Residuals vs. Fits Plot",
               "Q-Q Plot",
               "Structure of the data" ),
           "Fixing Violations" = 
             c("Transformations",
               "Weighted Least Squares",
               "Robust Regression",
               "Generalized Least Squares" )
           )
```


```{r Observations}
mod_table2 <- data.frame("Unusual Observations" = 
             c("Leverage",
               "Outliers",
               "Influential Points"),
             "Calculated" = 
               c("$\\small\\underbrace{H_{ii} = h_i}_{\\text{diagonals of the hat matrix}}$",
                 "$\\underbrace{r_i = \\frac{e_i}{s_e\\sqrt{1 - h_i}}}_{\\text{standardized residual}}$",
                 "$\\underbrace{D_i = \\frac{1}{p}r_i^2\\frac{h_i}{1-{h_i}}}_{\\text{Cook's Distance}}$")
           )
```



```{r}
kable(mod_table1, col.names = c("Assumptions", " Checking Assumptions", "Fixing Violated")) %>%
  collapse_rows(1, valign = "bottom") %>%
  kable_styling(full_width = FALSE, font_size = 12, bootstrap_options = c("striped"), position = "float_left") %>%
  column_spec(1, color = "slategray")

kable(mod_table2, col.names = c("Unusual Observations", "Calculated As")) %>%
  kable_styling(full_width = FALSE, font_size = 12, bootstrap_options = c("striped"), position = "left") %>%
  column_spec(1, color = "slategray")
```



-----------------------------



## References







