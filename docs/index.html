<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Linear Modeling in R</title>

<script src="site_libs/header-attrs-2.6/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


<link rel="stylesheet" href="assets/styles1.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "Óâô";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "Óâô";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Statistical Linear Models</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Linear Models in R</a>
</li>
<li>
  <a href="01-index.html">Model Diagnostics</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Linear Modeling in R</h1>

</div>


<div id="random-matrices" class="section level2">
<h2>Random Matrices:</h2>
<p>In Linear Modeling, we begin by discussing Matrix Rules and Properties and learning about <strong>Excepted Values</strong> and <strong>Covariances</strong>, and <strong>Variances</strong>, all of which are Random Matrices.</p>
<p><span class="sp5">a.</span> <strong>Expectation</strong> describes the average value</p>
<p><span class="math display">\[E[X] = \begin{cases}
\sum_x xp_X(x) &amp; X \textrm{ is a discrete random variable}\\
\int_{-\infty}^{\infty}xf_X(x)dx &amp; X \textrm{ is a continuous random variable}
\end{cases}\]</span></p>
<p><span class="sp5">b.</span> <strong>Covariance</strong> is a measure of joint variability between two random variables</p>
<p><span class="math display">\[cov(\mathbf{X},\mathbf{Y}) = E[(\mathbf{X} - E[\mathbf{X}])(\mathbf{Y} - E[\mathbf{Y}])]\]</span></p>
<p><span class="sp5">c.</span> <strong>Variance</strong> measures the spread of a variable</p>
<p><span class="math display">\[var(\mathbf{X}) = cov(\mathbf{X}, \mathbf{X}) = E[\mathbf{X}^2] - E[\mathbf{X}]^2\]</span></p>
<div id="least-squares-estimation" class="section level3">
<h3>Least Squares Estimation:</h3>
<p>The purpose of the <strong>least squares estimation</strong> is to estimate the relationship between various predictor variables <span class="math inline">\(X\)</span> and an outcome variable <span class="math inline">\(Y\)</span>. In using this relationship, we must understand the <strong>Law of Total Exception</strong> and <strong>Law of Total Variance</strong> rules.</p>
<p>¬†</p>
<div class="b1">
<p><span class="math display">\[\large \mathbf{y} = \mathbf{X} \beta + \epsilon\]</span></p>
</div>
<p>¬†</p>
<p>As a result, we can take the <strong>expected value</strong> of <span class="math inline">\(\hat \beta\)</span>, in which we want to show:</p>
<p><span class="math display">\[\mathbf{E[\hat{\beta}]=\beta}\]</span></p>
<p>In order to prove the above equation, we must use the Law of Total Exception and various Matrix rules. We can also take the <strong>variance of <span class="math inline">\(\hat \beta\)</span></strong>:</p>
<p><span class="math display">\[\rm{var}{(\mathbf{\hat \beta})} = \hat \sigma ^2 (\mathbf{X^TX})^{-1}\]</span></p>
<p>In order to get the variance of <span class="math inline">\(\hat \beta\)</span>, we have to calculate the variance/ covariance matrix using the properties of random matrices and the Law of Total Variance.</p>
<div id="summary" class="section level4">
<h4>Summary:</h4>
<p>The overall idea of the <strong>Law of Total Exception</strong> and the <strong>Law of Total Variance</strong> rules allow us to calculate the <strong>Expected Value</strong> and <strong>Variance</strong> or our main estimator <span class="math inline">\(\mathbf{\hat \beta}\)</span> from the <strong>Least Squares Estimator</strong>.</p>
<hr />
</div>
</div>
</div>
<div id="deriving-the-hat-matrix" class="section level2">
<h2>Deriving the Hat Matrix</h2>
<div id="linear-regression-review" class="section level3">
<h3>Linear Regression Review</h3>
<p><span class="math display">\[
\begin{align}
RSS &amp;= (\mathbf{y} - \mathbf{X}\hat\beta)^T(\mathbf{y}-\mathbf{X}\hat\beta) \\
&amp; = \mathbf{y}^T\mathbf{y}-\hat{\beta}^T\mathbf{X}^T\mathbf{y}-\mathbf{y}^T\mathbf{X}\hat\beta + \hat{\beta}^T\mathbf{X}^T\mathbf{X}\hat\beta\\
&amp;=\mathbf{y}^T\mathbf{y}-2\hat{\beta}^T\mathbf{X}^T\mathbf{y} + \hat{\beta}^T\mathbf{X}^T\mathbf{X}\hat\beta\\
\end{align}
\]</span></p>
<p>To find the <span class="math inline">\(\hat \beta\)</span> that is going to minimize this RSS,</p>
<p><span class="math display">\[\frac{\partial RSS}{\partial\hat\beta}=-2\mathbf{X}^T\mathbf{y}+2\mathbf{X}^T\mathbf{X}\hat\beta = 0\]</span></p>
<p>Solving for <span class="math inline">\(\hat \beta\)</span></p>
<p><span class="math display">\[
\begin{align}
-2\mathbf{X}^T\mathbf{y}+2\mathbf{X}^T\mathbf{X}\hat\beta &amp;= 0\\
2\mathbf{X}^T\mathbf{X}\hat\beta &amp; = 2\mathbf{X}^T\mathbf{y} \\
\mathbf{X}^T\mathbf{X}\hat\beta &amp; =\mathbf{X}^T\mathbf{y} \\
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\hat\beta &amp;=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\\
\underbrace{(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}}_{\mathbf{I}}\hat\beta &amp;=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\\
\mathbf{I}\hat\beta &amp;= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\\
\hat\beta &amp; = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{align}
\]</span></p>
<p><span class="math display">\[\large \hat\beta = {\bf (X^TX)^{-1} X^T \mathcal{y}}\]</span></p>
<p>¬†</p>
<p><strong>What is <span class="math inline">\(\hat \beta\)</span>?</strong></p>
<ul>
<li>The coefficient for <span class="math inline">\(x\)</span> is <span class="math inline">\(\hat \beta\)</span>. A one-unit increase in <span class="math inline">\(x\)</span> yields an expected increase in <span class="math inline">\(y\)</span> of <span class="math inline">\(\hat \beta\)</span> holding all other variables constant.</li>
</ul>
<p><strong>The hat matrix puts a üéì on y:</strong></p>
<p><span class="math display">\[
\begin{align}
\hat y &amp;= Hy \\
\hat y &amp;= \bf X \hat \beta \\
\hat{\mathbf{y}} &amp;= \underbrace{\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T}_{\mathbf{H}}\mathbf{y}
\end{align}
\]</span></p>
<p>¬†</p>
<p><span class="sp6">a.</span> Why is the hat matrix called the hat matrix?</p>
<p><span class="math display">\[
\begin{align*}
\hat y            &amp; = {\bf H} y \\
\bf{X} \hat \beta &amp; = {\bf H} y \\
{\bf X} \; (X^TX)^{-1} X^T y \; &amp; = {\bf H} y \\
X (X^TX)^{-1} X^T        &amp; = \bf H \\
{\bf I} &amp;= {\bf H}
\end{align*}
\]</span></p>
<p><span class="math display">\[
\begin{align}
&amp;\bf{I} = AA^{-1}  =
\tiny \begin{bmatrix}
{\bf 1} &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; {\bf 1} &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; {\bf 1} 
\end{bmatrix}
\end{align}
\]</span></p>
<p>¬†</p>
<hr />
</div>
</div>
<div id="bias" class="section level2">
<h2>Bias:</h2>
<p>Next, we learn what it means to be biased. In the following, we show how to prove that <span class="math inline">\(\hat \beta\)</span> is an <strong>unbiased estimator</strong>.</p>
<p>¬†</p>
<div class="b1">
<p><span class="math display">\[
\begin{align}
\rm{if} \quad \mathbf{E[\hat \beta - \beta]} &amp;= 0  \\
\\
\rm{then} \quad \large\mathbf{\hat\beta} &amp;\, \small\textrm{ is an unbiased estimator} 
\end{align}
\]</span></p>
</div>
<p>¬†</p>
<p>Hence, the above proves <span class="math inline">\(\hat \beta\)</span> is an unbiased estimator for the parameter <span class="math inline">\(\beta\)</span>.</p>
<p>Following this logic, it‚Äôs also important to discuss what it means to get the minimal variance. In doing so, we reuse the <span class="math inline">\(\mathrm{var}\mathbf{(\hat \beta)}\)</span> formula to establish the following inequality that defines what it means to be <span class="math inline">\({\color{Blue}{\texttt BLUE}}\)</span>.</p>
<div id="gauss-markov-theorem" class="section level3">
<h3>Gauss Markov Theorem:</h3>
<p><span class="math inline">\({\color{Blue}{\texttt BLUE}}\)</span> is the best linear unbiased estimator. We know the Least Squares Estimation returns the <strong>best linear unbiased estimator</strong> <span class="math inline">\(\mathbf{\hat \beta}\)</span>, but how can we <strong>prove</strong> this?</p>
¬†
<div class="b1">
<p><span class="math display">\[\large \textrm{var}(\color{Blue}{\hat\beta}) = \color{Teal}{\hat\sigma^2} (\mathbf{X}^T\mathbf{X})^{-1}\leq \textrm{var}(\color{HotPink}{\tilde\beta})\]</span></p>
</div>
<p>¬†</p>
<p>We want to prove what it means to be the <strong>best</strong> linear unbiased estimator. Hence, we want to show that <span class="math inline">\(\hat \beta\)</span> is better than some other parameter <span class="math inline">\(\tilde \beta\)</span> using the following inequality:</p>
<p><span class="math display">\[\mathrm{var}(\hat\beta)=\hat \sigma^2(X^TX)^{-1} \le{\mathrm{var}(\bar \beta)}\]</span></p>
<p>To solve this inequality, we need to use our information that we learned about <strong>bias</strong> to show that <span class="math inline">\(\tilde\beta\)</span> is unbiased:</p>
<p><span class="math display">\[\bf E[\bar\beta - \beta]=0\]</span></p>
<p>Next, in order to compare our <span class="math inline">\(\hat\beta\)</span> obtained from the <strong>least squares estimation</strong> to an unbiased linear estimator <span class="math inline">\(\tilde\beta\)</span>, we must show the variance of <span class="math inline">\(\tilde\beta\)</span> using our Variance/Covariance rules. As a result, we can show that our <span class="math inline">\(\hat \beta\)</span> variance is smaller or equal to the variance of <span class="math inline">\(\bar \beta\)</span> for all cases.</p>
<hr />
</div>
</div>
<div id="residual-sum-of-squares" class="section level2">
<h2>Residual Sum of Squares</h2>
<p>Next, we learned about the <strong>Residual Sum of Squares(RSS)</strong>, which is the sum of the residuals squared.</p>
¬†
<div class="b1">
<p><span class="math display">\[\large \mathbf{RSS}=\sum{e^2}\]</span></p>
</div>
<p>¬†</p>
<p>We use the Matrix Rules to be able to get various pieces of the RSS. For example, we showed the excepted value of the RSS:</p>
<p><span class="math display">\[E[e^Te]=\sigma^2(n-(p+1))\]</span></p>
<p>We got this using the Trace and idempotent rules.</p>
<p><span class="math display">\[E[e^Te]=E\mathbf{[y^T(I-H)y]}\]</span></p>
<p>When taking the expected value of the RSS, we used an expected value rule we already know about <span class="math inline">\(\mathbf{[y^T(I-H)y]}\)</span>. We also used that idempotent rule to reduce the <span class="math inline">\(I-H\)</span> term. The purpose of finding the residual sum of squares and its expected value is to get a good estimate for <span class="math inline">\(\hat \sigma^2\)</span>:</p>
<p><span class="math display">\[\hat \sigma^2 = \frac{RSS}{(n-(p+1))}\]</span></p>
<p>To derive this, we needed the fact that <span class="math inline">\(\mathbf{I-H}\)</span> is idempotent and the rule where we can take the expected value of a particular form of matrices that involves the <strong>trace</strong>. We then talk about what the trace is and how we can estimate it for the hat matrix as well for the identity matrix, which is how we got the <span class="math inline">\(\mathbf{(n-(p+1))}\)</span> piece. The <span class="math inline">\(\hat \sigma^2\)</span> value is an important piece for calculating the variance and covariance and variance/covariance matrix for the variance of <span class="math inline">\(\hat \beta\)</span>:</p>
¬†
<div class="b1">
<p><span class="math display">\[\mathrm{var}(\hat \beta)=\hat \sigma^2(X^TX)^{-1}\]</span></p>
</div>
<p>¬†</p>
<div id="rss-review" class="section level3">
<h3>RSS Review</h3>
<div class="b1">
<p><span class="math display">\[{\mathbf{e}^T\mathbf{e}=\mathbf{y}^T(\mathbf{I-H})\mathbf{y}}\]</span></p>
<p>We estimate <span class="math inline">\(\sigma^2\)</span> as follows as the following:</p>
<p><span class="math display">\[\hat\sigma^2 = \frac{\mathbf{e}^T\mathbf{e}}{n-(p+1)}\]</span></p>
<p>Next, we solve for <span class="math inline">\(E[e^Te]\)</span>:</p>
<p><span class="math display">\[
\begin{align}
E[\mathbf{e}^T\mathbf{e}] &amp;= \sigma^2(n-(p+1))\\
\hat\sigma^2&amp;=\frac{\mathbf{e}^T\mathbf{e}}{n-(p+1)}\\
&amp;=\frac{\mathbf{RSS}}{n-(p+1)}
\end{align}
\]</span></p>
</div>
<p>¬†</p>
<div id="standard-error" class="section level4">
<h4>Standard error</h4>
<p><strong>What is the Standard error?</strong> How much we expect the sample slope to vary from one random sample to another. The following estimates the <strong>standard error</strong> of a <span class="math inline">\(\beta\)</span> coefficient:</p>
<p><span class="math display">\[se(\hat\beta_{i-1})=\sqrt{(\mathbf{X}^T\mathbf{X})^{-1}_{ii}}\hat\sigma\]</span></p>
</div>
<div id="more-on-the-hat-matrix" class="section level4">
<h4>More on the Hat Matrix</h4>
<p><span class="math display">\[
\begin{align}\mathbf{e}^T\mathbf{e}=&amp;(\mathbf{y}-\hat{\mathbf{y}})^T(\mathbf{y}-\hat{\mathbf{y}})\\
=&amp;(\mathbf{y} - \mathbf{H}\mathbf{y})^T(\mathbf{y}-\mathbf{H}\mathbf{y})\\
=&amp;((\mathbf{I}-\mathbf{H})\mathbf{y})^T(\mathbf{I}-\mathbf{H})\mathbf{y}\\
=&amp;\mathbf{y}^T(\mathbf{I-H})^T(\mathbf{I}-\mathbf{H})\mathbf{y}\\
=&amp;\mathbf{y}^T(\mathbf{I-H})\mathbf{y}
\end{align}
\]</span></p>
<p><span class="math display">\[\underbrace{\small E[\mathbf{X}^T\mathbf{CX}]=E[\mathbf{X}]^T\mathbf{C}E[\mathbf{X}] +\textrm{tr}(\mathbf{C}\textrm{var}(\mathbf{X}))}_{\color{teal}{\star \;\textbf{matrix fact}}}\]</span></p>
<p><span class="math display">\[
\begin{align}
E[\mathbf{e}^T\mathbf{e}]=&amp;E[\mathbf{y}^T(\mathbf{I-H})\mathbf{y}]\\
=&amp;E[\mathbf{y}]^T(\mathbf{I-H})E[\mathbf{y}]+\textrm{tr}((\mathbf{I-H})\textrm{var}(\mathbf{y}))\\
=&amp;\beta^T\mathbf{X}^T(\mathbf{I-H})\mathbf{X}\beta+\sigma^2\textrm{tr}(\mathbf{I-H})\\
=&amp;\beta^T\mathbf{X}^T(\mathbf{X}\beta-\mathbf{H}\mathbf{X}\beta)+\sigma^2\textrm{tr}(\mathbf{I-H})\\
=&amp;\beta^T\mathbf{X}^T(\mathbf{X}\beta-\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\beta)+\sigma^2\textrm{tr}(\mathbf{I-H})\\
=&amp;\beta^T\mathbf{X}^T(\mathbf{X}\beta-\mathbf{X}\underbrace{(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}}_{\color{teal}{\large\mathbf{I}}}\beta)+\sigma^2\textrm{tr}(\mathbf{I-H})\\
=&amp;\beta^T\mathbf{X}^T(\mathbf{X}\beta-\mathbf{X}\beta)+\sigma^2\textrm{tr}(\mathbf{I-H})\\
=&amp;\beta^T\mathbf{X}^T(0)+\sigma^2\textrm{tr}(\mathbf{I-H})\\
=&amp;\sigma^2\textrm{tr}(\mathbf{I-H})\\
=&amp;\sigma^2(\textrm{tr}(\mathbf{I}_{n\times n})-\textrm{tr}(\mathbf{H}))\\
=&amp;\sigma^2(n-\textrm{tr}(\mathbf{H}))\\
=&amp;\sigma^2(n-(p+1))
\end{align}
\]</span></p>
<hr />
</div>
</div>
</div>
<div id="hypothesis-testing" class="section level2">
<h2>Hypothesis Testing</h2>
<p>When comparing different models, it‚Äôs often that we want to compare a small model to a larger one. We can now compare the RSS of the two models! In linear regression, the goal is to minimize the <span class="math inline">\(\mathbf{RSS}\)</span> and establish a model of fewest explanatory variables <span class="math inline">\(\mathbf{X_i}\)</span>. So if <span class="math inline">\(RSS_{small}-RSS_{larger}\)</span> is small, then the fit of the small model is almost as good as the larger one.</p>
<p><span class="sp6">a.</span> Is the model better than an intercept only model?</p>
<p>¬†</p>
<div class="b1">
<p><span class="math display">\[
\begin{align}
\mathbf{H_0}&amp;: \beta_1 = \beta_2 = {\ldots} = \beta_p = 0 \\
\mathbf{H_A}&amp;: \small\text{the models are not equal}
\end{align}
\]</span></p>
</div>
<p>¬†</p>
<p>Let‚Äôs create an F-Statistic so we can compare to the F-distribtion!</p>
<div id="f-statistic" class="section level3">
<h3>F-Statistic</h3>
<p><span class="math display">\[F = \frac{RSS_{small} - RSS_{larger} / (df_{small}- df_{larger})}{RSS_{larger}/df_{larger}}\sim F_{df_{small}- df_{larger}, df_{larger}}\]</span></p>
<p>When comparing two linear regression models, we use the <strong>F-test</strong> as a way to compare a small model to a large model. A big part of the F test is the residuals sum of squares. So it‚Äôs important to know how to get the RSS and its expected value. Based on the hypothesis testing, we can determine whether or not to fail to reject the null hypothesis (two models are the same) or reject our null hypothesis (two models are different).</p>
<div id="summary-1" class="section level4">
<h4>Summary:</h4>
<p><strong>F tests</strong> are important because they let us do <strong>hypothesis testing</strong>. The F-test gives us a way to systematically compare a small model to a large model. The hypothesis test gives us a way to see if the two models are equal, i.e.¬†if the small model is just as good as the large model. The F test is a built framework that uses the f-distribution, which can be used to find a good cut off for our hypothesis test to determine whether we can reject our null hypothesis that the models are the same.</p>
<p>¬†</p>
<p><strong>Exercise.</strong> If we‚Äôre only given the value of estimate <span class="math inline">\(\mathbf{\hat \sigma ^2}\)</span>, how can we get the <span class="math inline">\(\mathbf{RSS}\)</span>?</p>
<div class="a2">
<p>This example illustrates why we need an estimate of <span class="math inline">\(\hat \sigma^2\)</span> because, if we didn‚Äôt know <span class="math inline">\(\mathit{RSS}\)</span>, we do know that our estimate <span class="math inline">\(\hat \sigma^2 =\frac{\mathbf{RSS}}{(n-(p+1))}\)</span>, which we can then get <span class="math inline">\(\mathit{RSS}\)</span> by estimating <span class="math inline">\(E[e^Te]=\mathbf{\sigma^2}(n-(p+1))\)</span>.</p>
</div>
<hr />
</div>
</div>
</div>
<div id="example.-f-tests-in-r" class="section level2">
<h2>Example. F-tests in R</h2>
<p><strong>Goal.</strong> predict Salary from AtBat and Hits. Is this model better than an intercept only model?</p>
<pre class="r"><code>library(ISLR)
hitters_cc &lt;- Hitters[complete.cases(Hitters), ]</code></pre>
<p>¬†</p>
<p><strong>F-Statistic.</strong></p>
<p><span class="math display">\[\large{F = \frac{RSS_{small} - RSS_{larger} / (df_{small}- df_{larger})}{RSS_{larger}/df_{larger}}}\]</span></p>
<pre class="r"><code>small &lt;- lm(Salary ~ 1, data = hitters_cc)
larger &lt;- lm(Salary ~ AtBat + Hits, data = hitters_cc)</code></pre>
<p><strong>Get the RSS from this model.</strong></p>
<pre class="r"><code>rss_small &lt;- summary(small)$sigma^2 * (nrow(hitters_cc) - 1)
rss_larger &lt;- summary(larger)$sigma^2 * (nrow(hitters_cc) - 3)</code></pre>
<p><span class="sp7">a.</span> <strong>Method 1.</strong> Calculate it with given data</p>
<pre class="r"><code>f &lt;- ((rss_small - rss_larger) / 2) / 
  (rss_larger / (nrow(hitters_cc) - 3))
f
1 - pf(f, 2, nrow(hitters_cc) - 3)</code></pre>
<p>¬†</p>
<p><span class="sp7">b.</span> <strong>Method 2.</strong> Call the anova table</p>
<pre class="r"><code>anova(small,larger)</code></pre>
<p>¬†</p>
<p><strong>Testing One Variable.</strong> Find if the Hits variable an important contribution.</p>
<p><span class="math display">\[
H_0: \beta_2 = 0 \\
H_A: \beta_2 \neq 0
\]</span></p>
<pre class="r"><code>small &lt;- lm(Salary ~ AtBat, data = hitters_cc)
larger &lt;- lm(Salary ~ AtBat + Hits, data = hitters_cc)
anova(small, larger)</code></pre>
<p>¬†</p>
<p><span class="sp7">c.</span> <strong>Summary.</strong></p>
<pre class="r"><code>library(ISLR)
hitters_cc &lt;- Hitters[complete.cases(Hitters), ]
larger &lt;- lm(Salary ~ AtBat + Hits, data = hitters_cc)
summary1 &lt;- summary(larger)</code></pre>
<p>¬†</p>
<p><strong>What is the p-value?</strong> The probability of getting a statistic as extreme or more extreme than the observed test statistic given the null hypothesis is true.</p>
<hr />
</div>
<div id="confidence-intervals" class="section level2">
<h2>Confidence Intervals</h2>
<p><strong>Definition.</strong> If we use the same sampling method to select different samples and computed an interval estimate for each sample, we would expect the true population parameter (<span class="math inline">\(\beta_1\)</span>) to fall within the interval estimates 95% of the time.</p>
<p>¬†</p>
<div class="b1">
<p><span class="math display">\[\Large\hat\beta\pm t^*SE_{\hat\beta}\]</span></p>
</div>
<p>¬†</p>
<p><strong>Confidence intervals</strong> are a way to quantify our <strong>uncertainty</strong>. The confidence interval takes the <span class="math inline">\(\hat \beta\)</span> from our least squares estimation <span class="math inline">\(\pm\)</span> <span class="math inline">\(t^*\)</span>, which comes from our t-distribution, multiplied by the standard error of <span class="math inline">\(\hat \beta\)</span> <span class="math inline">\(\mathbf{SE_{\hat \beta}}\)</span>.</p>
<p><span class="math display">\[\hat\beta\pm t^*SE_{\hat\beta}\]</span></p>
<p>Suppose we select a different sample of size <span class="math inline">\(\mathcal{n}\)</span> from the same population and compute <span class="math inline">\(\mathcal{n}\)</span> new interval estimates. Each one of those <span class="math inline">\(\mathcal{n}\)</span> interval estimates would be different. We would expect that <strong>95%</strong> of those <span class="math inline">\(\mathcal{n}\)</span> would contain the true population parameter for that <span class="math inline">\(\beta\)</span> coefficient.</p>
<hr />
<div id="more-on-confidence-intervals" class="section level3">
<h3>More on Confidence Intervals</h3>
<p><strong>Run a simulation to generate 100 confidence intervals by sampling from a ‚Äútrue‚Äù population</strong></p>
<p>In the following example, we are interested in illustrating the relationship between <strong>Age</strong> and <strong>Wage</strong> such that <span class="math inline">\(\text{age} \approx \text{ Normal} (30,10)\)</span>, <span class="math inline">\(\epsilon \approx \text{ Normal}(0,10)\)</span>, and <span class="math inline">\(\text{sample } {n}=100\)</span>.</p>
<p>¬†</p>
<div class="b1">
<p><span class="math display">\[
\large
\underbrace{\text{wage} = {\color{teal}{\bf \beta_1}} \times \text{age} + \epsilon}_{\begin{matrix}{\bf \beta_1} \; \Rightarrow \; \text{true parameter for} \\ \text{ the relationship between} \\ \text{variables age and wage}.\end{matrix}}
\]</span></p>
</div>
<p>¬†</p>
<div id="generate-two-different-samples" class="section level4">
<h4>Generate Two Different Samples</h4>
<p>In the following, we generate a sample of 100 people (n). The <code>set.seed()</code> function makes it so that the method produces the same answer, and the <code>rnorm</code> function pulls a random normal variable.</p>
<p><strong>Sample 1:</strong></p>
<pre><code>##        Age      Wage
## 1 52.87247 110.93553
## 2 18.03228  41.93996
## 3 23.05707  45.32082
## 4 25.87707  40.01053
## 5 20.29327  43.67375
## 6 20.52720  25.01562</code></pre>
<p><strong>Sample 2:</strong></p>
<pre><code>##        Age      Wage
## 1 50.23344 105.71950
## 2 38.62492  74.49258
## 3 29.75091  60.04891
## 4 36.00635  68.13020
## 5 42.16481  80.15938
## 6 18.23468  24.82762</code></pre>
</div>
<div id="compare-intervals-to-the-true-parameter" class="section level4">
<h4>Compare intervals to the ‚Äútrue parameter‚Äù</h4>
<p>In the following, we use the <code>lm</code> function to fit two linear models using the two different samples we generated above. Next, we use the <code>confint</code> function to compute the confidence intervals of one or more parameters for each of the fitted models.</p>
<p><strong>Fit linear models:</strong></p>
<pre class="r"><code>Model &lt;- lm(Wage ~ Age, data = sample) ## Sample 1
Model2 &lt;- lm(Wage ~ Age, data = sample2) ## Sample 2</code></pre>
<p><strong>Compute the confidence intervals:</strong></p>
<p>¬†</p>
<pre><code>##                 2.5 %   97.5 %
## (Intercept) -5.513397 7.503540
## Age          1.811456 2.208256</code></pre>
<p>¬†</p>
<div class="a2">
<p><span class="math display">\[\texttt{95% CI}: (1.81, 2.21)\]</span> Our true parameter is <span class="math inline">\(\mathbf{2}\)</span>, which is a fixed number that falls within this above confidence interval estimate. Hence, sample 1 accurately captures an interval that contains the true parameter <span class="math inline">\(\mathbf{2}\)</span>.</p>
</div>
<p>¬†</p>
<pre><code>##                 2.5 %   97.5 %
## (Intercept) -8.880926 3.241064
## Age          1.885782 2.270270</code></pre>
<p>¬†</p>
<div class="a2">
<p><span class="math display">\[\texttt{95% CI}: (1.89, 2.27)\]</span> Our true parameter is <span class="math inline">\(\mathbf{2}\)</span>, which is a fixed number that falls within this above confidence interval estimate. Hence, sample 2 accurately captures an interval that contains the true parameter <span class="math inline">\(\mathbf{2}\)</span>.</p>
</div>
<p>¬†</p>
</div>
<div id="generate-100-confidence-intervals" class="section level4">
<h4>Generate 100 Confidence Intervals:</h4>
<p>In the following, we create the <code>get_ci</code> function to generate a random sample, fit a model predicting the relationship between <code>Wage</code> and <code>Age</code>, and return the confidence interval for the model. We then use the <code>purrr</code> package to run this simulation to generate 100 confidence intervals by sampling from a ‚Äútrue‚Äù population.</p>
<p>¬†</p>
<p>¬†</p>
</div>
<div id="plot-the-100-confidence-intervals" class="section level4">
<h4>Plot the 100 Confidence Intervals:</h4>
<p>In the following code, we use the <code>ggplot</code> function to plot 100 simulated intervals from above.</p>
<p><img src="index_files/figure-html/plot-1.png" width="450" style="display: block; margin: auto;" /></p>
<p>¬†</p>
<p><strong>What percent of the intervals contain the true parameter <span class="math inline">\(\mathbf{2}\)</span>?</strong></p>
<p><span class="math display">\[\frac{95 \small\text{ intervals}}{100\small\text{ intervals}}=95\texttt{%}\]</span></p>
<p>In the above exercise, we‚Äôre selecting 100 different samples and computing each of their interval estimates in order to get an approximation as close to the true population parameter. The figure we created above plots 100 simulated intervals represented by horizontal lines where the <span style="color:#001d47;"><strong>black</strong></span> lines represent the intervals that contain the true parameter <span class="math inline">\(\mathbf{2}\)</span> and the <span style="color:#ff66ad;"><strong>red</strong></span> lines represent the intervals that do <strong>not</strong> contain the true parameter. From the figure, we can calculate the percent of the intervals that contain the true parameter <span class="math inline">\(\mathbf{2}\)</span> by dividing all of the black lines by the total number of lines: <span class="math inline">\(\frac{95}{100}=95\texttt{%}\)</span>.</p>
<hr />
</div>
</div>
</div>
<div id="code-appendix" class="section level2">
<h2>Code Appendix</h2>
<pre class="r"><code>library(knitr)
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message = FALSE)
library(ISLR)
hitters_cc &lt;- Hitters[complete.cases(Hitters), ]
small &lt;- lm(Salary ~ 1, data = hitters_cc)
larger &lt;- lm(Salary ~ AtBat + Hits, data = hitters_cc)
rss_small &lt;- summary(small)$sigma^2 * (nrow(hitters_cc) - 1)
rss_larger &lt;- summary(larger)$sigma^2 * (nrow(hitters_cc) - 3)
f &lt;- ((rss_small - rss_larger) / 2) / 
  (rss_larger / (nrow(hitters_cc) - 3))
f
1 - pf(f, 2, nrow(hitters_cc) - 3)
anova(small,larger)
small &lt;- lm(Salary ~ AtBat, data = hitters_cc)
larger &lt;- lm(Salary ~ AtBat + Hits, data = hitters_cc)
anova(small, larger)
library(ISLR)
hitters_cc &lt;- Hitters[complete.cases(Hitters), ]
larger &lt;- lm(Salary ~ AtBat + Hits, data = hitters_cc)
summary1 &lt;- summary(larger)
library(purrr)
library(ggplot2)
set.seed(7)
n &lt;- 100 ## sample 100 people
sample &lt;- data.frame(
  Age = rnorm(n,  mean = 30,  sd = 10)
)
sample$Wage &lt;- 2 * sample$Age + rnorm(n, mean = 0, sd = 10)
head(sample)
n &lt;- 100
sample2 &lt;- data.frame(
  Age = rnorm(n, mean = 30, sd = 10)
)
sample2$Wage &lt;- 2 * sample2$Age + rnorm(n, 0, 10) 
head(sample2)
Model &lt;- lm(Wage ~ Age, data = sample) ## Sample 1
Model2 &lt;- lm(Wage ~ Age, data = sample2) ## Sample 2
confint(Model)
confint(Model2)
get_ci &lt;- function(id) {
  sample &lt;- data.frame(
    Age = rnorm(n, mean = 30, sd = 10)
  )
  sample$Wage &lt;- 2* sample$Age + rnorm(n, 0, 10)
  model &lt;- lm(Wage ~ Age, data = sample)
  return(
    data.frame(
      lb = confint(model)[2,1],
      ub = confint(model)[2,2],
      id = id
    ))}
## map function call 100 times
set.seed(7)
ci &lt;- map_df(1:100, get_ci)
ggplot(ci, aes(y = id, color = (lb &gt; 2 | ub &lt; 2))) + 
  geom_linerange(aes(xmin = lb, xmax = ub)) +
  geom_vline(xintercept = 2, lty=2) +
  scale_color_manual(values = c(&quot;black&quot;, &quot;#ff66ad&quot;)) +
  theme(legend.position = &quot;none&quot;)</code></pre>
<hr />
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references">
<div id="ref-dalpiazAppliedStatistics2016">
<p>[1] <span class="smallcaps">Dalpiaz</span>, D. (2016). <em>Applied Statistics with R</em>. STAT 420 at UIUC.</p>
</div>
<div id="ref-farawayLinearModels2004">
<p>[2] <span class="smallcaps">Faraway</span>, J. J. (2004). <em>Linear Models with R</em>. Chapman and Hall/CRC.</p>
</div>
<div id="ref-jamesIntroductionStatisticalLearning2013">
<p>[3] <span class="smallcaps">James</span>, G., <span class="smallcaps">Witten</span>, D., <span class="smallcaps">Hastie</span>, T. and <span class="smallcaps">Tibshirani</span>, R. (2013). <em>An Introduction to Statistical Learning</em>. vol 103 Springer New York, New York, NY.</p>
</div>
<div id="ref-mcgowan">
<p>[4] <span class="smallcaps">McGowan</span>, L. D. (2020). Linear models. <em>STA 312</em>.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
